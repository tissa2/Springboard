{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Contex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a DataFrame ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrame from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "l = [('Ankit',25),('Jalfaizy',22),('saurabh',20),('Bala',26)]\n",
    "rdd = sc.parallelize(l)\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "schemaPeople = sqlContext.createDataFrame(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(schemaPeople)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the DataFrame from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"how to read csv file\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.4'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170107-061401-recipeitems.json\r\n",
      "FremontBridge.csv\r\n",
      "Gold_2019_2020Sample.csv\r\n",
      "IntroAPIs.ipynb\r\n",
      "Logtransformation.ipynb\r\n",
      "MyFirstAssignment.ipynb\r\n",
      "Outliers.ipynb\r\n",
      "PredModelind Data Preprocessing and Feature Exploration .ipynb\r\n",
      "PySparkTutorial.ipynb\r\n",
      "README.md\r\n",
      "Tahir_CapstoneDataCollection.ipynb\r\n",
      "Tahir_DataWranglingPandas.ipynb\r\n",
      "Tahir_DatetimeDataExercise.ipynb\r\n",
      "Tahir_StringOperationsExercise.ipynb\r\n",
      "Tahir_TabularEDA.ipynb\r\n",
      "Tahir_TextEDA.ipynb\r\n",
      "Tahr_Capstone_DataCollection.ipynb\r\n",
      "Uber Data Analysis.ipynb\r\n",
      "Untitled.ipynb\r\n",
      "config_secret.json\r\n",
      "cv_stop.pkl\r\n",
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m/\r\n",
      "data_clean.pkl\r\n",
      "dtm.pkl\r\n",
      "dtm_stop.pkl\r\n",
      "iris.csv\r\n",
      "missing_values.ipynb\r\n",
      "omdb_api_data.json\r\n",
      "\u001b[1m\u001b[36mscrapy_mini_project\u001b[m\u001b[m/\r\n",
      "\u001b[1m\u001b[36mstrata_data-master\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('Gold_2019_2020Sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+-------+-------+------+--------+\n",
      "|       _c0|   _c1|    _c2|    _c3|    _c4|   _c5|     _c6|\n",
      "+----------+------+-------+-------+-------+------+--------+\n",
      "|      Date|  Open|   High|    Low|  Close|Volume|Currency|\n",
      "|2019-01-01|1284.7|1284.75|1282.85|1283.35|     0|     USD|\n",
      "|2019-01-02|1349.5| 1349.5| 1349.5| 1349.5|   100|     USD|\n",
      "|2019-01-03|1359.8| 1359.8| 1359.8| 1359.8|    10|     USD|\n",
      "|2019-01-04|1351.8| 1351.8| 1351.8| 1351.0|     1|     USD|\n",
      "+----------+------+-------+-------+-------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('Gold_2019_2020Sample.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+-------+-------+------+--------+\n",
      "|      Date|  Open|   High|    Low|  Close|Volume|Currency|\n",
      "+----------+------+-------+-------+-------+------+--------+\n",
      "|2019-01-01|1284.7|1284.75|1282.85|1283.35|     0|     USD|\n",
      "|2019-01-02|1349.5| 1349.5| 1349.5| 1349.5|   100|     USD|\n",
      "|2019-01-03|1359.8| 1359.8| 1359.8| 1359.8|    10|     USD|\n",
      "|2019-01-04|1351.8| 1351.8| 1351.8| 1351.0|     1|     USD|\n",
      "|2019-01-07|1355.2| 1355.2| 1355.2| 1355.2|     7|     USD|\n",
      "+----------+------+-------+-------+-------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to see datatype of columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Currency: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Show first n observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='2019-01-01', Open='1284.7', High='1284.75', Low='1282.85', Close='1283.35', Volume='0', Currency='USD'),\n",
       " Row(Date='2019-01-02', Open='1349.5', High='1349.5', Low='1349.5', Close='1349.5', Volume='100', Currency='USD'),\n",
       " Row(Date='2019-01-03', Open='1359.8', High='1359.8', Low='1359.8', Close='1359.8', Volume='10', Currency='USD'),\n",
       " Row(Date='2019-01-04', Open='1351.8', High='1351.8', Low='1351.8', Close='1351.0', Volume='1', Currency='USD'),\n",
       " Row(Date='2019-01-07', Open='1355.2', High='1355.2', Low='1355.2', Close='1355.2', Volume='7', Currency='USD')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+-------+-------+------+--------+\n",
      "|      Date|  Open|   High|    Low|  Close|Volume|Currency|\n",
      "+----------+------+-------+-------+-------+------+--------+\n",
      "|2019-01-01|1284.7|1284.75|1282.85|1283.35|     0|     USD|\n",
      "|2019-01-02|1349.5| 1349.5| 1349.5| 1349.5|   100|     USD|\n",
      "+----------+------+-------+-------+-------+------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2,truncate= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Count the number of rows in DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many columns do we have in train and test files along with their names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Currency'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns), df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get the summary statistics (mean, standard deviance, min ,max, count) of numerical columns in a DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+------------------+------------------+------------------+------------------+--------+\n",
      "|summary|      Date|              Open|              High|               Low|             Close|            Volume|Currency|\n",
      "+-------+----------+------------------+------------------+------------------+------------------+------------------+--------+\n",
      "|  count|       517|               517|               517|               517|               517|               517|     517|\n",
      "|   mean|      null|1603.4269825918761|1612.7332688588008| 1593.455319148936|  1603.56740812379| 49941.11411992263|    null|\n",
      "| stddev|      null|207.30519003183173|213.28042606164811|200.63091449840343|207.33926579441732|105630.21385324695|    null|\n",
      "|    min|2019-01-01|            1277.9|            1277.9|           1276.15|            1277.9|                 0|     USD|\n",
      "|    max|2020-12-11|            2076.4|            2089.2|            2049.0|            2069.4|              9960|     USD|\n",
      "+-------+----------+------------------+------------------+------------------+------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to select column(s) from the DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|      Date|  Close|\n",
      "+----------+-------+\n",
      "|2019-01-01|1283.35|\n",
      "|2019-01-02| 1349.5|\n",
      "|2019-01-03| 1359.8|\n",
      "|2019-01-04| 1351.0|\n",
      "|2019-01-07| 1355.2|\n",
      "+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Date','Close').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find the number of distinct date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('Date').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if I want to drop the all rows with null value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
